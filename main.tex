\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[margin=1.2in]{geometry}
\usepackage{listings}
\usepackage{mathtools}

\title{Natural Language processing}
\author{Edgar Montano}
\date{4 - 24 - 2016}

\begin{document}
\maketitle

\begin{abstract}
Natural language processing notes
\end{abstract}

\section{Regular Expressions and Automata (ch 2)}
\paragraph{Regular expression is used for specifying text strings in situation like web searches, also plays an important role in word processing, computation of frequencies from corpora, and other such tasks}
\paragraph{A string is a sequence of symbols, for the purpose of most text-based search techniques}
\paragraph{Regular expressions search require a \textbf{pattern} we want to search for, and a \textbf{corpus} of text to search through.  }
\subsection{Finite State Automata}
\paragraph{FSA are theoretical foundation of a good deal of the computational work, any regular expression can be implemented as a finite-state automaton.}
\paragraph{A regular expression is one way of characterizing a particular kind of formla language called a \textbf{regular language.} Both regular expressions and finite-state-automata can be used to describe regular languages. }
\paragraph{A \textbf{deterministic} algorithm is one that has no choice points. The algorithm always knows what to do for any input.  }
\paragraph{A \textbf{formal language} is a set of strings, each string composed of symbols from a finite symbol-set called an \textbf{alphabet}. }
\paragraph{Formal languages are not the same as \textbf{natural languages}, which are kind of languages that real people speak. }
\paragraph{The term \textbf{generative grammar }is sometimes used in linguistics to mean grammar of a formal language; the origin of the term is this use of an automaton to define a langauge by generating all possible strings. }
\subsection{Nondeterministic FSAs}
\paragraph{Automata with decision points are called \textbf{non-deterministic FSAs}. }


\section{Chomsky Normal Form}
\par{Cannot have more then two non-terminals on the right hand side}
\par{Rules:}
\par{if A -> B then we can rewrite this as A->XY and B->XY}
\par{To remove a production A-> BCDE, we can do the following: we keep the 1st variable constant and replace the last variables: so A-> BCDE, can be broken down to A->BX, X->CY, Y->DE}
\par{To remove rules where the right side constists of more than one terminal: To remove a production A->ab, we replace b with another variable: A-> aU, U->b}
\par{Remove rules where the right side consists of 1 variable and 1 terminal with 2 variables: A-> aU , G-> a, so A-> GU, and G->a}
\begin{itemize}
\item \textbf{Step 1}: make sure start symbol does not appear on righthand side
\item \textbf{Step 2:} remove rules Like A -> $\epsilon$
\item \textbf{Step 3:} get rid of all unit rules A-> B
\item \textbf{Step 4:} Get Rid of rules with more then 2 symbols on right hand side. 
\item \textbf{Step 5:} make sure only 2 must be variables(non terminals) if only one, it has to be non-terminal symbol. 

\end{itemize}

\section{tf idf}
\par{TF: Term frequency: measures how much freuqntly a term occurs in a document. $TF(t) = \frac{number of times term t appears in a doc}{total number of terms in the document}$}
\par{IDF: inverse document frequency: measures how important a term is.  $idf(t) = log_e(\frac{total number of documents}{number of documents with term t in it}$}


\section{Notes}
\subsection{Defininig Computational Linguistics}
\par{Definition: Study of how to solve problems involving the interpretation and generation of human language text and speech.}
\subsection{Role of manual annotation}
\par{Popular but imperfect measurement of agreement: $kappa = \frac{Percent(Actual Agreement) - Prob(chance of Agreement)}{1-Prob(chance Agreement)}$}
\par{Used to create answer keys to score system output, one  set of measures are: recall, precision and f-score: }
\par{$recall = \frac{|correct|}{|answer key|}$}
\par{$precision = \frac{|correct|}{|system output|}$}
\par{$f-score = \frac{1}{\frac{1}{2} * ( \frac{1}{precision} + \frac{1}{recall}  ) }$}

\subsection{Formal Grammar}
\par{A formal grammar defines a formal language}
A formal grammar consists of:
\begin{itemize}
\item \textbf{N}: a finite set of nonterminal symbols
\item T: a finite set of terminal symbols
\item R: a set of rewrite rules, XYZ -> abXzY
\item S: A special nonterminal that is the start symbol
\end{itemize}


\subsubsection{A simple example of a formal grammar}
Language AB = 1 or more a, followed by 1 or more b, e.g. ab, aab, abb, aaaaaaabb, etc.
\begin{itemize}
\item N = \{A,B\}
\item T = \{a,b\}
\item S = $\Sigma$
\item R = \{A->a, A->Aa, B->b, B->Bb, $\Sigma$->AB\}
\end{itemize}

\subsection{The Chomsky Hierarchy: Type 0 and 1}
\par{Type 0: No restrictions on rules}
\par{Type 1: Context-sensitive rules}
\par{Example: DUCK DUCK DUCK -> DUCK DUCK GOOSE. means convert DUCK to a GOOSE if preceded by 2 ducks}
\par{Complexity of recognizer for languages: $type 0 = expotential, type 1 = polynomial, type 2 = O(n^3) type 3 = O(nlogn)$s}

\subsection{RegExp}
\par{}

\subsection{Probabilisitc Assumptions of HMM Tagging}
\par{$P(x|y) = \frac{P(y|x)P(x)}{P(y)}$}

\subsection{TF-IDF}
\par{Term Frequency x Inverse Document Frequency}
\par{TF = frequency of term in corpus}
\par{IDF = num of docs / num of docs containing term}
\par{Example: 100 documents, 100 instances of word cat: if each in  a different doc: 100 x 100 / 100 = 100}
\par{inverse document frequency: reciprocal of portion of large document set that contain term t, normalized with log function: $log(\frac{NumberOfDocuments}{NumberOfDocumentsContantaining(T)})$}
\par{coconut milk, occurs 3 times. term frequency = 3. occurs in 4 out of 10, 000 documents in collection. inverse document frequency = $log(10,000/4) = 7.82$}
\par{$tfidf = 3 * 7.82 = 23.46$}

\subsection{Cosine Similarity: Common Similarity Score}
\par{$Similarity(A,B) = \frac{\Sigma_i a_i x b_i}{\sqrt[]{\Sigma_i (ai)^2 x \Sigma_i (bi)^2} }$}

\subsection{What is a named entity?}
\par{Definition 1: a single or multi-word expression that meets any of the following criteria: is a proper noun phrase, is a proper adjective phrase, has external distribution of NP, but different internal structure}
\par{Definition 2: a class of words and multi-word expressions defined by specifications tuned to information extraction tasks (can conflict with 1 by including normal nouns) }

\subsection{Reference Resolution}
\par{Anaphora: is a word phrase that refers back to another phrase: the antecedent of the anaphor: Mary thought that she lost her keys.}
\par{cataphora (less common): a cataphor is a word/phrase that refers forward to another phrase: its precdent: She was at NYU, when Mary realized that she lost her keys}




\end{document}